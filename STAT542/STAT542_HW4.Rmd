---
title: "Homework 4, STAT 542"
author: "Wenke Huang"
output:
  pdf_document: default
  html_document: default
fontsize: 12pt
geometry: margin=0.6in
---

## Question 1

### 1.1 Quick Introduction

In this case, we would like to fit the slided inverse regression which is discussed in the lecture not `SIR`. Here, we are about to choose the numbers of slices $H = 10$ which is a fixed number and will not be tuned here. The code we use to generate the data set has been offered by Professor Zhu. We will use this data set to perform our following analysis. 

```{r, echo=FALSE, message=FALSE}
set.seed(1)

require(MASS)
require(dr)
require(CCA)

n = 500
p = 10
V = 0.5^abs(outer(1:p, 1:p, "-"))  

x = mvrnorm(n, runif(p), V)

b1 = matrix(c(sqrt(0.5), 0, sqrt(0.5), 0, rep(0, p-4)))
b2 = matrix(c(0, sqrt(0.5), 0, -sqrt(0.5), rep(0, p-4)))

dir1 = x %*% b1
dir2 = x %*% b2

link = dir1 + 4*atan(2*dir2)
y = link + 0.5*rnorm(n)

# use the dr package, and perform SIR using 10 slices 

fit.sir = dr(y~., data = data.frame(x, y), method = "sir", nslices = 10)
# fit.sir$evectors[, 1:2]
# cc(fit.sir$evectors[, 1:2], cbind(b1, b2))$cor
```

### 1.2 Model Fitting

First, we calculate the sample covariance matrix $\hat{\Sigma}_x$ for the data set $\mathbf{X}$. Second, we subtract column means from the $\mathbf{X}$ to make each variable mean 0. Then, we multiply $\hat{\Sigma}_x^{-1/2}$ on the right. Here, we can get ourselves a new matrix $\mathbf{Z}$. Sort the dataset ($\mathbf{Z}$, $\mathbf{Y}$) by the observed Y values. Divide the dataset (according to the sorted values of Y) into H slices as equally as possible. In this case, we can easily find out that each slice will be assigned to 50 (which is equal to 500/10) observations evenly. Within each slice $h$ ($h$ = 1,2,...,10), we compute the sample column mean of $Z_h$. We also compute the sample column mean $\bar{z}$ of our data set $Z$. Then, we compute the covariance matrix for the slice means of Z, weighted by the slice sizes: $\mathbf{M} = \displaystyle{\frac{\sum_{h=1}^{H}{n_h (\bar{z}_h-\bar{z}) (\bar{z}_h-\bar{z})^T}}{n}}$. Finally, we perform PCA on $\mathbf{M}$ and obtain the K largest eigenvectors, $\hat{\alpha}_1$, $\hat{\alpha}_2$,..., $\hat{\alpha}_K$. After we get them, we transform them back to $\hat{\beta}_1=\hat{\Sigma}_x^{-1/2}\hat{\alpha}_1$, $\hat{\beta}_2=\hat{\Sigma}_x^{-1/2}\hat{\alpha}_2$,..., $\hat{\beta}_K=\hat{\Sigma}_x^{-1/2}\hat{\alpha}_K$. In this case, we set $K$ to be 2.

```{r, echo=FALSE, message=FALSE}
set.seed(1)
# Step 1
CovMatrix <- stats::cov(x)
Z <- base::scale(x, center = colMeans(x), scale = FALSE) %*% base::solve(expm::sqrtm(CovMatrix))

# Step 2
ZY <- base::cbind(Z,y)
SortOrder <- base::order(ZY[,11], decreasing = FALSE, na.last = TRUE)
Sorted.ZY <- ZY[SortOrder,]
z <- base::array(0, dim = c(n/10, 11, 10))
for (i in 1:10){
  z[, , i] <- Sorted.ZY[(n/10*(i-1)+1): (n/10*i), ]
}

# Step 3
zhbarY <- base::array(0, dim = c(10, 11))
for (i in 1:10){
  zhbarY[i, ] <- colMeans(z[, , i])
}
zhbar <- t(zhbarY[, -11])
zbarY <- as.vector(colMeans(Sorted.ZY))
zbar <- zbarY[-11]

# Step 4
m.new <- base::array(0, dim = c(10, 10))
m.previous <- base::array(0, dim = c(10, 10))
for(i in 1:10){
  m.new = (as.vector(zhbar[, i]) -as.vector( zbar)) %*% t(as.vector(zhbar[, i]) - as.vector(zbar))* (1/10)
  m.new = m.new + m.previous
  m.previous = m.new
}
M <- m.new

# Step 5
PCA <- stats::prcomp(M)
beta1.beta2 <- (solve(expm::sqrtm(CovMatrix)) %*% PCA$rotation)[,1:2]
# cc(beta1.beta2, cbind(b1,b2))$cor


# Visualize the result
library(knitr)
BETA1.BETA2 <- cbind(data.frame(beta1.beta2), data.frame(fit.sir$evectors[, 1:2]))
names(BETA1.BETA2) <- c("Dir1.own", "Dir2.own", "Dir1.dr", "Dir2.dr")
knitr::kable(BETA1.BETA2,
             align = "c",
             caption = "Results of both Own and Package Codes")
```

### 1.3 Further Test and Conclusion

The above table include results from both our own and `dr()` codes. However, we find that they don't match perfectly. Here, we will use an another method to test our own codes.

First of all, we generate a new set of data by changing line 6 of the code to $n = 50000$. Then, we perform our code on the data with number of slices $H=10$ and obtain the first two directions $\hat{\beta}_1$ and $\hat{\beta}_2$, which is same with the previous part. In the next step, we perform a canonical correlation analysis (CCA) between the true directions `cbind(b1, b2)` (one 10 Ã— 2 matrix) and your estimated directions `cbind`($\hat{\beta}_1$, $\hat{\beta}_2$). If our estimated directions form the same column space as the true directions, then the canonical correlations should be very close to 1. The results will be shown below.

```{r, echo=FALSE}
set.seed(1)
n = 50000
p = 10
V = 0.5^abs(outer(1:p, 1:p, "-"))  

x = mvrnorm(n, runif(p), V)

b1 = matrix(c(sqrt(0.5), 0, sqrt(0.5), 0, rep(0, p-4)))
b2 = matrix(c(0, sqrt(0.5), 0, -sqrt(0.5), rep(0, p-4)))

dir1 = x %*% b1
dir2 = x %*% b2

link = dir1 + 4*atan(2*dir2)
y = link + 0.5*rnorm(n)

CovMatrix <- stats::cov(x)
Z <- base::scale(x, center = colMeans(x), scale = FALSE) %*% base::solve(expm::sqrtm(CovMatrix))

# Step 2
ZY <- base::cbind(Z,y)
SortOrder <- base::order(ZY[,11], decreasing = FALSE, na.last = TRUE)
Sorted.ZY <- ZY[SortOrder,]
z <- base::array(0, dim = c(n/10, 11, 10))
for (i in 1:10){
  z[, , i] <- Sorted.ZY[(n/10*(i-1)+1): (n/10*i), ]
}

# Step 3
zhbarY <- base::array(0, dim = c(10, 11))
for (i in 1:10){
  zhbarY[i, ] <- colMeans(z[, , i])
}
zhbar <- t(zhbarY[, -11])
zbarY <- as.vector(colMeans(Sorted.ZY))
zbar <- zbarY[-11]

# Step 4
m.new <- base::array(0, dim = c(10, 10))
m.previous <- base::array(0, dim = c(10, 10))
for(i in 1:10){
  m.new = (as.vector(zhbar[, i]) -as.vector(zbar)) %*% t(as.vector(zhbar[, i]) - as.vector(zbar))* (1/10)
  m.new = m.new + m.previous
  m.previous = m.new
}
M <- m.new

# Step 5
PCA <- stats::prcomp(M)
beta1.beta2 <- (solve(expm::sqrtm(CovMatrix)) %*% PCA$rotation)[,1:2]

N50000 <- data.frame(t(cc(beta1.beta2, cbind(b1,b2))$cor))
row.names(N50000) <- "Canonical Correlations"
names(N50000) <- c("cc1", "cc2")
knitr::kable(N50000,
             align = "c",
             caption = "Results of CCA (n=50000)")
```

We find that the canonical correlations are extremely close to 1. We can conclude that our code should be correct. Then, we change $n$ back to 500. This time, we will perform `dr()` and our own personal code on the data set. Then we perform CCA between the first two directions obtained by both two methods.

```{r, echo=FALSE}
set.seed(1)
n = 500
p = 10
V = 0.5^abs(outer(1:p, 1:p, "-"))  

x = mvrnorm(n, runif(p), V)

b1 = matrix(c(sqrt(0.5), 0, sqrt(0.5), 0, rep(0, p-4)))
b2 = matrix(c(0, sqrt(0.5), 0, -sqrt(0.5), rep(0, p-4)))

dir1 = x %*% b1
dir2 = x %*% b2

link = dir1 + 4*atan(2*dir2)
y = link + 0.5*rnorm(n)

CovMatrix <- stats::cov(x)
Z <- base::scale(x, center = colMeans(x), scale = FALSE) %*% base::solve(expm::sqrtm(CovMatrix))

# Step 2
ZY <- base::cbind(Z,y)
SortOrder <- base::order(ZY[,11], decreasing = FALSE, na.last = TRUE)
Sorted.ZY <- ZY[SortOrder,]
z <- base::array(0, dim = c(n/10, 11, 10))
for (i in 1:10){
  z[, , i] <- Sorted.ZY[(n/10*(i-1)+1): (n/10*i), ]
}

# Step 3
zhbarY <- base::array(0, dim = c(10, 11))
for (i in 1:10){
  zhbarY[i, ] <- colMeans(z[, , i])
}
zhbar <- t(zhbarY[, -11])
zbarY <- as.vector(colMeans(Sorted.ZY))
zbar <- zbarY[-11]

# Step 4
m.new <- base::array(0, dim = c(10, 10))
m.previous <- base::array(0, dim = c(10, 10))
for(i in 1:10){
  m.new = (as.vector(zhbar[, i]) -as.vector( zbar)) %*% t(as.vector(zhbar[, i]) - as.vector(zbar))* (1/10)
  m.new = m.new + m.previous
  m.previous = m.new
}
M <- m.new

# Step 5
PCA <- stats::prcomp(M)
beta1.beta2 <- (solve(expm::sqrtm(CovMatrix)) %*% PCA$rotation)[,1:2]


N500 <- data.frame(t(cc(fit.sir$evectors[, 1:2], beta1.beta2)$cor))
row.names(N500) <- "Canonical Correlations"
names(N500) <- c("cc1", "cc2")
knitr::kable(N500,
             align = "c",
             caption = "Results of CCA (n=500)")
```

The above table shows us that the canonical correlations are close to 1. Hence, we can conclude that our code is correct in this case.

## Question 2

### 2.1 Quick Introduction

In this case, we will perform boosting on the email span dataset data(spam) in the `ElemStatLearn` package. Both exponential and logistic likelihood loss will be discussed, and number of trees and shrinkage factor will be tuned. The possible values of `n.trees` will be `300, 350` and `400` while the possible values of `shrinkage` will be `0.01, 0.1` and `0.99`. 10-folds cross-validation will be used to find out the best tuning parameters and a better model of these two. The possible tuning parameters `interaction.depth` and `n.minobsinnode` are set to be fixed values `1` and `10`, respectively.

### 2.2 Model Fitting

Boosting produces a sequence of learniers: $F_T(x)=\displaystyle\sum_{t=1}^Tf_t(x)$. At the t-th iteration, given previously estimated $f_1, f_2,...f_{t-1}$, we estimate a new function $h(x)$ to minimize the loss: $\displaystyle\text{min}\sum_{i=1}^n L(y_i, \sum_{k=1}^{t-1}f_k(x_i)+h(x_i))$. Boosting is an additive model, but its slightly different from generalized additive model, in which each weak learner only involves one variable, and only p number of functions are used and added up. In boosting, each $f_t(x)$ may involve all variables in general, and we may fit a large number ($T$) of functions.

AdaBoost is a special case of this framework with `Exponential loss` for classification. First, we initiate the weights $w_i^{(1)} =1/n,i=1,2,...,n$. For $t=1$ to $T$, we repeat the following procedures (a) - (d). (a), Fit a classifier $f_t(x) \in \{-1, 1\}$ to the weighted data, with individual weights $w_i^{(t)}$. (b) Compute $\epsilon_t=\sum_i w_i^{(t)}\mathbf{1}\{y_i \neq f_t(x_i)\}$. (c) Compute $\alpha_i = \frac{1}{2} log \frac{1-\epsilon_t}{\epsilon_t}$. (d) Then we update the weights, $w_i^{(t+1)}=\frac{w_i^{(t)}}{Z_t}exp\{-\alpha_t y_i f_t(x_i)\}$. Our final model will be $F_T(x)=\sum_{t=1}^T \alpha_t f_t(x)$. The rule of output the classification is: $sign(F_T(x))$.

For logistic likelihood loss, we assume $y$ is the label of data and $x$ is a feature vector. The classification framework can be formalized as: $\text{arg min} \displaystyle\sum_iL(y_i, f(\mathbf{x}_i))$, where $f$ is a hypothesis function and $L$ is loss function. For logsitic regression, we have the following instantiation: $f(\mathbf{x}) = \beta^T \mathbf{x}$, $L(y, f(\mathbf{x})) = \text{log}(1 + \text{exp}(-yf(\mathbf{x})))$ where $y \in \{\pm 1\}$.

```{r, echo=FALSE, message=FALSE, eval = TRUE}
set.seed(1)
library(ElemStatLearn)
Spam <- ElemStatLearn::spam

library(gbm)
library(caret)

caretGrid <- expand.grid(interaction.depth=1, n.trees = c(300, 350, 400),
                         shrinkage=c(0.01, 0.1, 0.99),
                         n.minobsinnode=10)
trainControl <- trainControl(method="cv", number=10)

boosting.Logistic <- train(
  spam ~ .,
  data = Spam,
  method = "gbm",
  distribution = "adaboost",
  trControl = trainControl,
  tuneGrid = caretGrid,
  verbose = FALSE
)

boosting.Exponential <- train(
  spam ~ .,
  data = Spam,
  method = "gbm",
  distribution = "bernoulli",
  trControl = trainControl,
  tuneGrid = caretGrid,
  verbose = FALSE
)
```

### 2.3 Comparation and Conclusion

```{r, echo=FALSE, eval=TRUE}
set.seed(1)
Exponential.result <- data.frame(boosting.Exponential$results)[,c(1,4,5)]
names(Exponential.result) <- c("shrinkage.exp", "ntrees.exp", "Accuracy.exp")
row.names(Exponential.result) <- NULL
Logistic.result <- data.frame(boosting.Logistic$results)[,c(1,4,5)]
names(Logistic.result) <- c("shrinkage.log", "ntrees.log", "Accuracy.log")
row.names(Logistic.result) <- NULL
knitr::kable(cbind(Exponential.result, Logistic.result),
             align = "c",
             caption = "Results of Model Fitting")

BestTune <- data.frame(
  rbind(
    boosting.Exponential$bestTune[c(1,3)],
    boosting.Logistic$bestTune[c(1,3)]
  ),
  c("Exponential Loss", "Logistic Likelihood Loss"),
  c(max(boosting.Exponential$results$Accuracy),
    max(boosting.Logistic$results$Accuracy))
)

row.names(BestTune) <- NULL
names(BestTune)[c(3,4)] <- c("Loss", "Accuracy")
knitr::kable(BestTune,
             align = "c",
             caption = "Best Tuning Parameters of Both Methods")
```

The results of both of the two methods are shown above. `.exp` indicates `Exponential Loss` while `.log` indicates `Logistic Likelihood Loss`. It is obvious that the method boosting with `Exponential Loss` is better in this case because its cross-validation accuracy is a little higher than the other one. And its corresponding best tuning parameters of `n.trees` and `shrinkage` are `r BestTune[1, 1]` and `r BestTune[1, 2]`, respectively. What's more, there are still some other parameters like `interaction.depth` and `n.minobsinnode` which are set to be fixed values which have been discussed ahead in this case. 
